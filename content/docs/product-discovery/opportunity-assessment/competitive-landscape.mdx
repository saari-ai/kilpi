---
title: Competitive Landscape
description: Mapping competitors, understanding differentiation, and assessing competitive intensity
---

Competitive landscape analysis identifies who else is solving (or could solve) the same problem. Discovery is not just about whether the problem exists — it is about whether there is room for your solution in a market where others are already trying.

## Competitor types

Competition is broader than "companies that look like us." Map all three types:

### Direct competitors

Companies solving the same problem for the same segment with a similar approach.

| Attribute | What to capture |
|-----------|----------------|
| Company/product | Name and description |
| Target segment | Who they serve (compare to your target) |
| Approach | How they solve the problem |
| Pricing | Price points and model (subscription, usage, etc.) |
| Traction | Users, revenue, funding, growth signals |
| Strengths | What they do well |
| Weaknesses | Where they fall short (from user reviews, your research) |

### Indirect competitors

Companies solving the same underlying problem but with a different approach or for a different segment.

Examples:
- Spreadsheets are an indirect competitor to nearly every SaaS tool
- Consultants are indirect competitors to self-serve software
- Doing nothing (tolerating the problem) is always an indirect competitor

### Jobs-to-be-done competition

Products or behaviours that compete for the same job, even if they look nothing like your product category. This is based on the Jobs to Be Done framework — people "hire" products to get a job done, and they will fire one product for another that does the job better.

**Example:** A customer who needs to "feel confident their product is secure" might hire:
- A security compliance tool (direct competitor)
- A penetration testing consultancy (indirect competitor)
- A blog post with a security checklist (JTBD competitor)
- An enterprise customer's security questionnaire as a forcing function (JTBD competitor)

Understanding JTBD competition reveals the real alternatives your customers are choosing between — which may not be the companies you assumed were your competitors.

## Competitive mapping

Summarise your competitive analysis in a comparison table:

| Competitor | Type | Segment | Approach | Price | Strength | Weakness |
|-----------|------|---------|----------|-------|----------|----------|
| *e.g., Vanta* | *Direct* | *SMB-Enterprise* | *Automated compliance* | *$$$* | *Strong automation, integrations* | *Expensive for solos, compliance-focused not risk-focused* |
| *e.g., Security checklist blog* | *JTBD* | *Solo devs* | *Free content* | *Free* | *Accessible, no commitment* | *Generic, not tailored to specific stack* |
| *e.g., Doing nothing* | *JTBD* | *All* | *Accept the risk* | *Free* | *Zero effort* | *Works until first enterprise customer asks for compliance* |
| | | | | | | |

## Differentiation opportunities

Look for gaps in the competitive landscape where existing solutions underserve the target segment. Common differentiation vectors:

| Vector | Question | Evidence source |
|--------|----------|-----------------|
| Segment underserved | Is your target segment ignored or poorly served by existing solutions? | Customer research — do users complain about existing solutions not fitting them? |
| Problem underserved | Do existing solutions solve a different aspect of the problem? | Interview data — what do users still struggle with despite using competitors? |
| Approach gap | Is there a fundamentally different way to solve this that competitors have not tried? | Observation data — what workarounds do users build? |
| Price gap | Are existing solutions priced out of reach for your target segment? | Market data — what does your segment currently spend? |
| Distribution gap | Can you reach the target segment through a channel competitors do not use? | Market research — where does your segment discover and evaluate tools? |

## Competitive intensity scoring

Score competitive intensity for the [Opportunity Scorecard](/docs/product-discovery/templates/opportunity-scorecard):

| Score | Label | Conditions |
|-------|-------|------------|
| 5 | Blue ocean | No direct competitors. Problem is being solved with workarounds or not at all. |
| 4 | Low competition | 1-2 direct competitors, none dominant. Significant differentiation opportunities. |
| 3 | Moderate competition | 3-5 direct competitors. Differentiation possible but not obvious. |
| 2 | High competition | Multiple well-funded competitors. Differentiation requires a strong wedge. |
| 1 | Red ocean | Dominant incumbent(s) with high switching costs and strong network effects. |

Most real opportunities score 2-4. A score of 5 is rare and should prompt skepticism — if no one is solving this problem, there may be a reason. A score of 1 is a strong signal to kill or pivot unless you have a genuinely disruptive approach.

## Switching costs

When evaluating competition, assess how hard it is for customers to switch from existing solutions:

| Switching cost type | Example | Impact on competition |
|--------------------|---------|----------------------|
| Data migration | Customer data locked in competitor's format | High — users stay even if unhappy |
| Workflow disruption | Team has built processes around existing tool | Medium — requires compelling reason to change |
| Learning curve | Team has invested in training on existing tool | Medium — new tool must be easier or vastly better |
| Contract lock-in | Annual contract with competitor | Low-Medium — timing-dependent |
| Network effects | Value increases with number of users on the platform | Very high — nearly insurmountable without a wedge strategy |

High switching costs mean you need a significantly better solution, not just a marginally better one. Factor this into your feasibility assessment.
